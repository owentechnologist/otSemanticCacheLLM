# to run this example you will need the following :
""" 
An active enpoint running localAI as a REST service
Example:
curl http://34.148.227.54:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
  "model": "qwen2.5-0.5b-instruct",
  "messages": [{"role": "user", "content": "Recommend a nursery rhyme for a 4 year old girl."}],
  "temperature": 0.3
}'
"""

# sample execution: > 
""" 
python simpleLLM_with_cache.py -h <host> -p <port> [optional -s <password>] [optional -u <username>] 
python3 simpleLLM_with_cache.py -h redis-18000.homelabwin.local -p 18000
python3 simpleLLM_with_cache.py -h redis-10000.re-cluster1.ps-redislabs.org -p 10000
"""

# redis imports: for caching prompts and responses and 
# searching using Vector Similarity for previous prompts 

from redis.commands.search.field import VectorField
from redis.commands.search.query import Query
from redis.commands.search.indexDefinition import IndexDefinition, IndexType
# import to allow easy HTTP calls:
import requests
# import to allow manipulation of json and jsonpath navigation of response from LLM:
import json, jsonpath_ng.ext as jsonpath
#LLM related imports:
from sentence_transformers import SentenceTransformer
#general imports: 
import time,uuid
### cmdline_utils==General Setup &
# UI and Redis connection functions: ###
from cmdline_utils import *


# when generating keys to be included in our search index
# as well as the name of our search index we use this string
# to avoid collisions with other data and indexing unnecessary data:
uid_prefix = "ot11"

''' 
The string generated by this function is used as the keyname for 
the Object in Redis that holds the user prompt text and 
points to the cached response.
The actual KeyName is not important (except for its prefix)
As we are using Redis Query Engine and all such objects are being
indexed.  
'''
def string_for_keyname():
    s = str(uuid.uuid4())
    return(uid_prefix+':prompt:'+s)

### LLM / AI Setup ###
# Q: where is the LLM library? A: we are using a hosted 'localAI' server
# https://localai.io/ 
llm_chat_url = "http://FIXME:8080/v1/chat/completions"
        
# prep for Search index creation logic 
# - most invocations won't need to create a new index:
create_new_index = False

# this function creates the index in redis and returns its name:
def create_index_in_redis(redis_connection):
    index_name = 'idx_vss_lclllm_'+uid_prefix
    SCHEMA = [
        VectorField("embedding", "FLAT", {"INITIAL_CAP": 1000, "TYPE": "FLOAT32", "DIM": 768, "DISTANCE_METRIC": "COSINE"}),
    ]
    # Create the index (Index is created in the Redis Database)
    try:
        redis_connection.ft(index_name).create_index(fields=SCHEMA, definition=IndexDefinition(prefix=[(uid_prefix+":prompt:")], index_type=IndexType.HASH))
    except Exception as e:
        print(repr(e))
    return index_name

# this function executes the VSS search call against Redis
# it accepts a redis index (generated by calling connection)
def vec_search(vindex,query_vector_as_bytes):
    # KNN 10 specifies to return only up to 10 nearest results (could be unrelated)
    # the small VECTOR_RANGE specifies the prompts must be very similar
    query =(
        Query(f'(@embedding:[VECTOR_RANGE .0102 $vec_param]=>{{$yield_distance_as: range_dist}})=>[KNN 10 @embedding $knn_vec]=>{{$yield_distance_as: knn_dist}}')
        .sort_by(f'knn_dist') #asc is default order
        .return_fields("response_key", "knn_dist")
        .dialect(2)
    )
    res = vindex.search(query, query_params = {'vec_param': query_vector_as_bytes, 'knn_vec': query_vector_as_bytes})
    return res.docs

## This function is where we interact with the LLM 
# - providing a prompt that guides the behavior as well as 
# the question posed by the user:
def ask_llm(question):
    # a little prompt engineering is needed to get the answers in a usable format:
    template_=f"""The prompt that follows is a question you must answer in a friendly way. 
        Prompt:  {question} 
        Begin...
        """    
    # HERE IS WHERE YOU COULD PASTE IN A DIFFERENT template_:


    llm_request_data = {"model": "qwen2.5-1.5b-instruct","response_format": {"type": "json"}, "messages": [{"role": "user", "content": f"{template_}"}], "temperature": 0.25}
    print(f"DEBUG: we are sending this to the LLM:\n {llm_request_data}")
    headers =  {"Content-Type": "application/json"}    
    myResponse = requests.post(llm_chat_url,json=llm_request_data,headers=headers )
    decoded_json = myResponse.content.decode('utf-8')
    json_data = json.loads(decoded_json)
    print(f"\nDEBUG: {json_data.keys()}")    

    # provide a default string for the reply in case LLM fails:
    response_s = "Unknown (Not Answered)"

    # Specify the path to be used within the JSON returned from the LLM: 
    json_query = jsonpath.parse("choices[0].message.content")
    # thought: how could we use jsonpath.parse to extract the count of tokens used?

    ## extract the value located at the path we selected in our json_query
    for match in json_query.find(json_data):
        response_s=match.value

    return response_s

# initialize index in Redis (will not break if we attempt again)
index_name = create_index_in_redis(redis_connection=redis_connection)

# UI loop: (if user responds with "END" - program ends)
while True:

    ## useful examples:
    # https://blog.baeke.info/2023/03/21/storing-and-querying-for-embeddings-with-redis/ 
    # https://github.com/RediSearch/RediSearch/blob/master/docs/docs/vecsim-range_queries_examples.ipynb
    sentences = [display_menu()]
    user_input = sentences
    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
    # create the numpy array encoding representation of the sentences/prompt:
    embeddings = model.encode(sentences)
    # convert the embedding numpy array to bytes for use in Redis Hash:
    embedding_vector_as_bytes = embeddings.tobytes()

    # pass the user text/prompt to the LLM Chain and print out the response: 
    # also, check for a suitable cached response
    if user_input:
        # we are interested in seeing how long it takes to use Vectors: 
        start_time=time.perf_counter()

        # assign the details of the hash value to be stored: 
        prompt_hash = {
            "prompt_used": sentences[0],
            "embedding": embedding_vector_as_bytes,
            "response_key": ""
        }

        # Before we search for a semantically similar prompt,
        # we need to make sure we store the hash in Redis with 
        # this latest prompt:
        """ Uncomment the following line to enable Semantic caching """
        # redis_connection.hset(name=string_for_keyname(), mapping=prompt_hash)

        # now we can search for semantically similar prompt(s)
        results = vec_search(redis_connection.ft(index_name),embedding_vector_as_bytes)
        # only write the response to prompt hashes in redis with response_key=""
        # due to the sortby this should always at least be the one at zero index
        # we expect at most 10 docs (KNN 10 is specified in query)
        # they are in asc distance order - smaller the distance the better
        found_useful_result = False
        llm_response = ""
        for next_result in results:
            if not next_result.response_key == "":
                # ensure that we only reuse a response that is suitable
                # cached responses are stored as strings in redis 
                # and the keyname for the relevant string is stored in the 
                # response attribute of the prompt Hash object in Redis.
                # This decouples the storage of the answer allowing for more reuse
                # answers stored this way can easily be edited and all similar prompts will 
                # point to this now, updated response|answer
                if (float(next_result.knn_dist) < .2) and (next_result.response_key != "") :
                    print(f'\nFound a match! -->\n {next_result}\n')
                    found_useful_result = True
                    # llm_reponse_cache_key should point to a string in redis:
                    llm_response_cache_key=next_result.response_key
                    print(f'keyname of cached response: {llm_response_cache_key}')
                    llm_response=redis_connection.get(llm_response_cache_key)
            if found_useful_result: 
                # write the nearby result as the response to this prompt in redis
                # in this way, multiple prompts may share the same result
                redis_connection.hset(results[0].id,'response_key',llm_response_cache_key)
                break
        # we have exhausted all the potential matches:
        if not found_useful_result:
            print('\n No suitable response has been cached.  Generating new Response...\n')
            # create a new LLM-generated result as the answer:            
            llm_response = ask_llm(user_input) 
            x = redis_connection.incr(f'{uid_prefix}:prompt:responsekeycounter')
            # store the full response in a string in redis 
            redis_connection.set(f'{uid_prefix}:prompt:response:{x}',llm_response)
            # write the cached response keyname to the response attribute in redis:
            # due to sorting of the results by KNN distance ASC the first result should be our target:
            redis_connection.hset(results[0].id,'response_key',(f'{uid_prefix}:prompt:response:{x}'))            
        
        # output whatever the result is to the User Interface:
        print(f'{spacer}\n{llm_response}{spacer}\n')
        uparrows = " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n"
        print(f'\t{uparrows}\tElapsed Time to respond to user prompt was: {(time.perf_counter()-start_time)*1} seconds\n')
