# to run this example you will need the following :
""" 
An active enpoint running localAI as a REST service
Example:
curl http://34.148.227.54:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
  "model": "qwen2.5-0.5b-instruct",
  "messages": [{"role": "user", "content": "Recommend a nursery rhyme for a 4 year old girl."}],
  "temperature": 0.3
}'
"""

# sample execution: > 
""" 
python chatbot1.py -h <host> -p <port> [optional -s <password>] [optional -u <username>] 
python3 chatbot1.py -h redis-12000.homelab.local -p 12000
python3 chatbot1.py -h redis-10000.re-cluster1.ps-redislabs.org -p 10000
"""
# redis imports: for caching prompts and responses and 
# searching using Vector Similarity for previous prompts 
import redis
from redis.commands.search.field import VectorField
from redis.commands.search.query import Query
from redis.commands.search.indexDefinition import IndexDefinition, IndexType
# import to allow easy HTTP calls:
import requests
# import to allow manipulation of json and jsonpath navigation of response from LLM:
import json, jsonpath_ng.ext as jsonpath
#LLM related imports:
from sentence_transformers import SentenceTransformer
#general imports: 
import time,sys,getopt,re

### General Setup / functions: ###
## this function is designed to return a 'likely to be unique' value for a string
## a bloom or cuckoo filter used for deduping (to guarantee uniqueness) would be better
def compact_string_for_keyname(payload):
    payload_string1 = payload[0]
    payload_string1 = payload_string1.replace(" ", "")
    sumchars=0
    for s in payload_string1:
        sumchars = sumchars+ord(s)
    response = f'{re.sub("[}aeiouAEIOU{?!,.]","",payload_string1)}:{sumchars}'
    return(response)

spacer = "\n**********************************************\n"

def display_menu():
    #display something to UI CMDLine:
    print(spacer)
    print('\tType: END   and hit enter to exit the program...\n')
    print('\tCommandline Instructions: \nType in your prompt/question as a single statement with no return characters... ')
    print('(only hit enter for the purpose of submitting your question)')
    print(spacer)
    # get user input/prompt/question:
    user_text = input('\n\tWhat is your question? (prompt):\t')
    if user_text =="END" or user_text =="end":
        print('\nYOU ENTERED --> \"END\" <-- QUITTING PROGRAM!!')
        exit(0)
    return (user_text)
        
### Redis Setup / functions: ###

## checks sys.args for host and port etc...
redis_host = 'USER-PROVIDED'#'redis-10000.re-cluster1.ps-redislabs.org'
redis_port = 10000
redis_password = ""
redis_user = "default"
create_new_index = False

argv = sys.argv[1:] # skip the name of this script
opts,args = getopt.getopt(argv,"h:p:s:u:", 
                                ["host =",
                                "port =",
                                "password =",
                                "username =",
                                ]) 
for opt,arg in opts:
    if opt in ['-h','-host']:
        redis_host = arg
    elif opt in ['-p','-port']:
        redis_port = arg
    elif opt in ['-s','-secret_password']:
        redis_password = arg
    elif opt in ['-u','-username']:
        redis_user = arg
if len(sys.argv)<4:
    print('\nPlease supply a hostname & port for your target Redis instance:\n')
    print('\n\tYour options are: host port password username:')
    print('-h <host> -p <port> -s <password> -u <username>')
    exit(0)


if redis_password == "" and redis_user == "default":
    redis_connection = redis.Redis( host=redis_host, port=redis_port, encoding='utf-8', decode_responses=True)
else:
    redis_connection = redis.Redis( host=redis_host, port=redis_port, password=redis_password ,username=redis_user, encoding='utf-8', decode_responses=True)

# this function creates the index in redis and returns its name:
def create_index_in_redis(redis_connection):
    index_name = 'idx_vss_lclllm_ot11'
    SCHEMA = [
        VectorField("embedding", "FLAT", {"INITIAL_CAP": 1000, "TYPE": "FLOAT32", "DIM": 768, "DISTANCE_METRIC": "COSINE"}),
    ]
    # Create the index
    try:
        redis_connection.ft(index_name).create_index(fields=SCHEMA, definition=IndexDefinition(prefix=["ot11:prompt:"], index_type=IndexType.HASH))
    except Exception as e:
        print(repr(e))
    return index_name

# this function executes the VSS search call against Redis
# it accepts a redis index (generated by calling connection)
def vec_search(vindex,query_vector_as_bytes):
    # KNN 10 specifies to return only up to 10 nearest results (could be unrelated)
    # the small VECTOR_RANGE specifies the prompts must be very similar
    query =(
        Query(f'(@embedding:[VECTOR_RANGE .02 $vec_param]=>{{$yield_distance_as: range_dist}})=>[KNN 10 @embedding $knn_vec]=>{{$yield_distance_as: knn_dist}}')
        .sort_by(f'knn_dist') #asc is default order
        .return_fields("response_key", "knn_dist")
        .dialect(2)
    )
    res = vindex.search(query, query_params = {'vec_param': query_vector_as_bytes, 'knn_vec': query_vector_as_bytes})
    return res.docs

### LLM / AI Setup ###
# Q: where is the LLM library? A: we are using a hosted 'localAI' server
# https://localai.io/
llm_chat_url = "http://35.237.73.4:8080/v1/chat/completions"

def ask_llm(question):
    # a little prompt engineering is needed to get the answers in a usable format:
    template_=f"""The prompt that follows is a question you must answer in a friendly way. 
        Prompt:  {question} 
        Begin...
        """    
    # HERE IS WHERE YOU COULD PASTE IN A DIFFERENT template_:

    llm_request_data = {"model": "qwen2.5-0.5b-instruct","response_format": {"type": "json"}, "messages": [{"role": "user", "content": f"{template_}"}], "temperature": 0.1}
    print(f"DEBUG: we are sending this to the LLM:\n {llm_request_data}")
    headers =  {"Content-Type": "application/json"}    
    myResponse = requests.post(llm_chat_url,json=llm_request_data,headers=headers )
    decoded_json = myResponse.content.decode('utf-8')
    json_data = json.loads(decoded_json)
    print(f"\nDEBUG: {json_data.keys()}")    

    response_s = "Unknown (Not Answered)"

    query = jsonpath.parse("choices[0].message.content")
    for match in query.find(json_data):
        response_s=match.value

    return response_s

user_input = "BEGIN"
index_name = create_index_in_redis(redis_connection=redis_connection)

# UI loop: (if user responds with "END" - program ends)
while True:

    ## useful examples:
    # https://blog.baeke.info/2023/03/21/storing-and-querying-for-embeddings-with-redis/ 
    # https://github.com/RediSearch/RediSearch/blob/master/docs/docs/vecsim-range_queries_examples.ipynb
    sentences = [display_menu()]
    user_input = sentences
    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
    # create the numpy array encoding representation of the sentences/prompt:
    embeddings = model.encode(sentences)
    # convert the embedding numpy array to bytes for use in Redis Hash:
    embedding_vector_as_bytes = embeddings.tobytes()

    # pass the user text/prompt to the LLM Chain and print out the response: 
    # also, check for a suitable cached response
    if user_input:
        start_time=time.perf_counter()
        trimmed_question = compact_string_for_keyname(sentences)
        prompt_hash = {
            "prompt_used": sentences[0],
            "embedding": embedding_vector_as_bytes,
            "response_key": ""
        }

        if not redis_connection.exists(f"ot11:prompt:{trimmed_question}"):
            redis_connection.hset(name=f"ot11:prompt:{trimmed_question}", mapping=prompt_hash)

        results = vec_search(redis_connection.ft(index_name),embedding_vector_as_bytes)
        # only write the response to an empty prompt: hashkey
        # due to the sortby this should always be the one at zero index
        # we expect at most 10 docs (KNN 10 is specified in query)
        # they are in asc distance order - smaller the distance the better
        found_useful_result = False
        llm_response = ""
        for next_result in results:
            if not next_result.response_key == "":
                # ensure that we only reuse a response that is suitable
                # cached responses are stored as strings in redis 
                # and the keyname for the relevant string is stored in the 
                # response attribute of the prompt Hash object in Redis.
                # This decouples the storage of the answer allowing for more reuse
                # answers stored this way can easily be edited and all similar prompts will 
                # point to this now, updated response|answer
                if (float(next_result.knn_dist) < .2) and (next_result.response_key != "") :
                    print(f'\nFound a match! -->\n {next_result}\n')
                    found_useful_result = True
                    # llm_reponse_cache_key should point to a string in redis:
                    llm_response_cache_key=next_result.response_key
                    print(f'keyname of cached response: {llm_response_cache_key}')
                    llm_response=redis_connection.get(llm_response_cache_key)
            if found_useful_result: 
                # write the nearby result as the answer to this object in redis
                # in this way, multiple prompts will share the same result
                redis_connection.hset(results[0].id,'response_key',llm_response_cache_key)
                break
        # we have exhausted all the potential matches:
        if not found_useful_result:
            print('\n No suitable response has been cached.  Generating new Response...\n')
            # create a new AI-generated result as the answer:            
            llm_response = ask_llm(user_input) 
            x = redis_connection.incr('ot11:prompt:responsekeycounter')
            # store the full response in a string in redis under the keyname prompt:response:x
            redis_connection.set(f'ot11:prompt:response:{x}',llm_response)
            # write the cached response keyname to the response attribute in redis:
            # due to sorting of the results by KNN distance ASC the first result should be our target:
            redis_connection.hset(results[0].id,'response_key',(f'ot11:prompt:response:{x}'))            
        
        # output whatever the result it to the User Interface:
        print(f'{spacer}{llm_response}{spacer}\n')
        uparrows = " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n"
        print(f'\t{uparrows}\tElapsed Time to respond to user prompt was: {(time.perf_counter()-start_time)*1} seconds\n')
